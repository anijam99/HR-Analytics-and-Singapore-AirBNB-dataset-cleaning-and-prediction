{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HR Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORIGINAL HR ANALYTICS DATASET FROM ASG1\n",
    "hr_data = pd.read_csv('hr_data.csv')\n",
    "hr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import feature_engine.imputation as mdi\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Missing Value Imputation Copied from Asg1\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imp_num_median', mdi.MeanMedianImputer(imputation_method = 'median', variables='previous_year_rating')),\n",
    "    ('imp_cat_frequent', mdi.CategoricalImputer(variables ='education', imputation_method='frequent')),\n",
    "])\n",
    "\n",
    "pipe.fit(hr_data)\n",
    "hr_data = pipe.transform(hr_data)\n",
    "\n",
    "#Categorical Encoding Copied from Asg1\n",
    "hr_data['education'] = hr_data['education'].replace((\"Master's & above\",\n",
    "                                                     \"Bachelor's\", \"Below Secondary\"),(3, 2, 1))\n",
    "ohe_enc = OneHotEncoder(\n",
    "    top_categories=None,  \n",
    "    variables=None,\n",
    "    drop_last=False)\n",
    "ohe_enc.fit(hr_data)\n",
    "hr_data = ohe_enc.transform(hr_data)\n",
    "\n",
    "#Scaling of Data to exclude is_promoted column\n",
    "df_target = hr_data['is_promoted']\n",
    "df_scale = hr_data.drop(columns=['is_promoted'])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_scale)\n",
    "df_hr_scaled = scaler.transform(df_scale)\n",
    "df_hr_sScaled = pd.DataFrame(df_hr_scaled, columns=df_scale.columns)\n",
    "df_hr_sScaled = df_hr_sScaled.reset_index(drop=True)\n",
    "df_target = df_target.reset_index(drop=True)\n",
    "hr_data = df_hr_sScaled\n",
    "hr_data['is_promoted'] = df_target\n",
    "\n",
    "#Additional features Copied from Asg1\n",
    "hr_data['total_score'] = hr_data['avg_training_score'] * hr_data['no_of_trainings']\n",
    "hr_data['awards_per_year'] = hr_data['awards_won?'] / hr_data['length_of_service']\n",
    "\n",
    "#Dropping features Copied from Asg1\n",
    "hr_data = hr_data.drop(['employee_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr = hr_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Stratified Sampling\n",
    "hr1=df_hr[df_hr['is_promoted']==1]\n",
    "hr0=df_hr[df_hr['is_promoted']==0]\n",
    "hr0_sampled = hr0.sample(n=len(hr1), random_state=2).copy()\n",
    "hr_stratified=pd.concat([hr1,hr0_sampled],axis=0)\n",
    "x_train_strat, x_test_strat, y_train_strat, y_test_strat = train_test_split(hr_stratified.drop('is_promoted', axis= 1),\n",
    "                                                    hr_stratified['is_promoted'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stratified Promoted:', (hr_stratified['is_promoted'] == 1).sum())\n",
    "print('Stratified Not promoted:', (hr_stratified['is_promoted'] == 0).sum())\n",
    "print(x_train_strat.shape, x_test_strat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "x=df_hr.drop('is_promoted', axis= 1)\n",
    "y=df_hr['is_promoted']\n",
    "X, Y = smote.fit_resample(x, y)\n",
    "x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Smote Promoted:', (Y == 1).sum())\n",
    "print('Smote Not promoted:', (Y == 0).sum())\n",
    "print(x_train_smote.shape, x_test_smote.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build the Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression baseline model test for Stratified Sampling and SMOTE Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_strat = LogisticRegression()\n",
    "lr_strat.fit(x_train_strat, y_train_strat)\n",
    "lr_smote = LogisticRegression()\n",
    "lr_smote.fit(x_train_smote, y_train_smote)\n",
    "print('Stratified training accuracy is: ', lr_strat.score(x_train_strat,y_train_strat))\n",
    "print('Stratified testing accuracy is: ', lr_strat.score(x_test_strat,y_test_strat))\n",
    "print('SMOTE training accuracy is: ', lr_smote.score(x_train_smote,y_train_smote))\n",
    "print('SMOTE testing accuracy is: ', lr_smote.score(x_test_smote,y_test_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_smote\n",
    "x_test = x_test_smote\n",
    "y_train = y_train_smote\n",
    "y_test = y_test_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "results  = cross_validate(lr, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(x_train, y_train)\n",
    "\n",
    "results  = cross_validate(dtc, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "\n",
    "results  = cross_validate(rf, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost with Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "base_model = dtc\n",
    "ada = AdaBoostClassifier(estimator=base_model)\n",
    "ada.fit(x_train, y_train)\n",
    "\n",
    "results  = cross_validate(ada, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cbg = CatBoostClassifier()\n",
    "cbg.fit(x_train, y_train)\n",
    "\n",
    "results  = cross_validate(cbg, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(x_train, y_train)\n",
    "\n",
    "results  = cross_validate(gbc, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[('rf', rf), ('ada', ada), ('cat', cbg)], voting='soft')\n",
    "ensemble.fit(x_train, y_train)\n",
    "\n",
    "results  = cross_validate(ensemble, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evaluate and Improve the Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy and Cross Validaton Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_list = [lr, dtc, rf, ada, cbg, gbc, ensemble]\n",
    "model_name = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"CatBoost\", \"GradientBoost\",\"Soft Voting\"]\n",
    "j=0\n",
    "for i in model_list:\n",
    "    print(model_name[j], ' training accuracy is: ', i.score(x_train,y_train))\n",
    "    print(model_name[j], ' testing accuracy is: ', i.score(x_test,y_test))\n",
    "    print(model_name[j], ' cross val training accuracy is:', sum(results_list[j]['train_score'])/len(results_list[j]['train_score']))\n",
    "    print(model_name[j], ' cross val testing accuracy is:', sum(results_list[j]['test_score'])/len(results_list[j]['test_score']))\n",
    "    print()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "dtc = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n",
    "              \"max_depth\": [20, 30, 35, 40], \n",
    "              \"min_samples_leaf\" : [2, 4, 6, 8]}\n",
    "\n",
    "gs = GridSearchCV(estimator=dtc, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "gs = gs.fit(df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'])\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(criterion = 'gini', max_depth = 20, min_samples_leaf = 8, random_state=1)\n",
    "dtc.fit(x_train, y_train)\n",
    "print('training accuracy is: ', dtc.score(x_train,y_train))\n",
    "print('testing accuracy is: ', dtc.score(x_test,y_test))\n",
    "\n",
    "results  = cross_validate(dtc, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_features='sqrt', random_state=1, n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n",
    "              \"max_depth\": [20, 30, 35, 40], \n",
    "              \"min_samples_leaf\" : [2, 4, 6, 8], \n",
    "              \"n_estimators\": [40, 70, 100, 120]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'])\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion = 'entropy', max_depth = 40, min_samples_leaf = 2, n_estimators=70, random_state = 1)\n",
    "rf.fit(x_train,y_train)\n",
    "\n",
    "print('training accuracy is: ', rf.score(x_train,y_train))\n",
    "print('testing accuracy is: ', rf.score(x_test,y_test))\n",
    "\n",
    "results  = cross_validate(rf, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbg = CatBoostClassifier(random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "        'iterations': [500, 750, 1000],\n",
    "        'depth': [3, 6, 9],\n",
    "        'learning_rate': [0.1, 0.2, 0.25],\n",
    "        'l2_leaf_reg': [1, 3, 5]}\n",
    "\n",
    "gs = GridSearchCV(estimator=cbg, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "gs = gs.fit(df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'])\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbg = CatBoostClassifier(depth = 3, iterations = 1000, l2_leaf_reg = 3, learning_rate = 0.1, random_state=1)\n",
    "cbg.fit(x_train, y_train)\n",
    "\n",
    "results  = cross_validate(cbg, df_hr.drop('is_promoted', axis= 1), df_hr['is_promoted'], scoring='accuracy', cv=5, return_train_score = True)\n",
    "results_list.append(results)\n",
    "print('training accuracy is: ', cbg.score(x_train,y_train))\n",
    "print('testing accuracy is: ', cbg.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [dtc, rf, cbg]\n",
    "model_name = [\"Decision Tree\", \"Random Forest\", \"CatBoost\"]\n",
    "j=0\n",
    "for i in model_list:\n",
    "    print(model_name[j], ' training accuracy is: ', i.score(x_train,y_train))\n",
    "    print(model_name[j], ' testing accuracy is: ', i.score(x_test,y_test))\n",
    "    print(model_name[j], ' cross val training accuracy is:', sum(results_list[j]['train_score'])/len(results_list[j]['train_score']))\n",
    "    print(model_name[j], ' cross val testing accuracy is:', sum(results_list[j]['test_score'])/len(results_list[j]['test_score']))\n",
    "    print()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best HR Analytics Model - CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prices = pd.DataFrame({'Actual': y_test, 'Prediction': cbg.predict(x_test)})\n",
    "predicted_prices.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Airbnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = pd.read_csv('listings_new.csv')\n",
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airbnb = airbnb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airbnb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_airbnb.drop('price',axis=1), \n",
    "                                                    df_airbnb['price'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(x_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm = SVR()\n",
    "svm.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor()\n",
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "cat_reg = CatBoostRegressor()\n",
    "cat_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "lgbm = LGBMRegressor()\n",
    "lgbm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate and Improve the Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "model_list = [lr, rf_reg, svm, mlp, cat_reg, lgbm,xgb]\n",
    "model_name = [\"Linear Regression\", \"Random Forest\", \"SVM\", \"MLP\", \"CatBoost\", \"LightGBM\", \"XGBoost\"]\n",
    "j=0\n",
    "for i in model_list:\n",
    "    print(model_name[j], ' training mean squared error is: ', mean_squared_error(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing mean squared error is: ', mean_squared_error(i.predict(x_test), y_test))\n",
    "    print(model_name[j], ' training mean absolute error is: ', mean_absolute_error(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing mean absolute error is: ', mean_absolute_error(i.predict(x_test), y_test))\n",
    "    print(model_name[j], ' training r-square is: ', r2_score(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing r-square is: ', r2_score(i.predict(x_test), y_test))\n",
    "    print()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Airbnb Dataset Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "airbnb = pd.read_csv('listings.csv')\n",
    "df_airbnb = airbnb.copy()\n",
    "# Unecessary features to predict price\n",
    "df_airbnb.drop(['id','name','host_id','host_name','last_review','reviews_per_month'], inplace=True, axis=1)\n",
    "# Cap outliers from target value (price)\n",
    "def cap_outliers(df, column):\n",
    "    lower_quantile = df[column].quantile(0.25)\n",
    "    upper_quantile = df[column].quantile(0.75)\n",
    "    df[column] = df[column].apply(lambda x: lower_quantile if x < lower_quantile else x)\n",
    "    df[column] = df[column].apply(lambda x: upper_quantile if x > upper_quantile else x)\n",
    "    return df\n",
    "#df_airbnb = cap_outliers(df_airbnb,'price')\n",
    "outlier = (np.abs(stats.zscore(df_airbnb[\"price\"]))<0.8)\n",
    "outlier_ix = np.where(outlier==False)\n",
    "df_airbnb.drop(index=outlier_ix[0], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "outliers = [] \n",
    "# Check outliers using zscore\n",
    "for lat in df_airbnb['latitude']:\n",
    "    zscore = (lat - np.mean(df_airbnb['latitude'])) / np.std(df_airbnb['latitude'])\n",
    "    if zscore > 3:\n",
    "        outliers.append(lat)\n",
    "# replace outliers with median\n",
    "df_cleaned = df_airbnb.replace(outliers, np.median(df_airbnb['latitude']))\n",
    "# Hosts at air-bnb provide a maximum of one year stay (365 days) in the form of rent to the visitors\n",
    "df_cleaned = df_cleaned[df_airbnb['minimum_nights'] <= 365]\n",
    "\n",
    "#Using log transformation on target variable price, will use np.expm1() on predicted price\n",
    "df_cleaned['price'] = np.log1p(df_cleaned['price'])\n",
    "#Numeric Transform\n",
    "df_cleaned['minimum_nights']=np.log1p(df_cleaned['minimum_nights'])\n",
    "df_cleaned['availability_365']=np.log1p(df_cleaned['availability_365'])\n",
    "df_cleaned['calculated_host_listings_count']=np.log1p(df_cleaned['calculated_host_listings_count'])\n",
    "df_cleaned['number_of_reviews']=np.log1p(df_cleaned['number_of_reviews'])\n",
    "\n",
    "\n",
    "# Map integers to categorical values\n",
    "room_dict ={\n",
    "    'Entire home/apt': 1,\n",
    "    'Private room': 2,\n",
    "    'Shared room': 3\n",
    "}\n",
    "df_cleaned['room_type'] = df_cleaned['room_type'].copy().map(room_dict)\n",
    "\n",
    "# Label encoding for neighbourhood group and neighbourhood\n",
    "label = LabelEncoder()\n",
    "df_cleaned['neighbourhood_group'] = label.fit_transform(df_cleaned['neighbourhood_group'])\n",
    "label = LabelEncoder()\n",
    "df_cleaned['neighbourhood'] = label.fit_transform(df_cleaned['neighbourhood'])\n",
    "\n",
    "df_airbnb=df_cleaned\n",
    "# Scaling the data\n",
    "df_target = df_airbnb['price']\n",
    "df_scale = df_airbnb.drop(columns=['price'])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_scale)\n",
    "airbnb_scaled = scaler.transform(df_scale)\n",
    "airbnb_sScaled = pd.DataFrame(airbnb_scaled, columns=df_scale.columns)\n",
    "airbnb_sScaled = airbnb_sScaled.reset_index(drop=True)\n",
    "df_target = df_target.reset_index(drop=True)\n",
    "df_airbnb = airbnb_sScaled\n",
    "df_airbnb['price'] = df_target\n",
    "df_airbnb.info()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_airbnb.drop('price',axis=1), \n",
    "                                                    df_airbnb['price'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(x_train, y_train.ravel()) \n",
    "svm = SVR()\n",
    "svm.fit(x_train, y_train.ravel())\n",
    "mlp = MLPRegressor()\n",
    "mlp.fit(x_train,y_train)\n",
    "cat_reg = CatBoostRegressor()\n",
    "cat_reg.fit(x_train,y_train)\n",
    "lgbm = LGBMRegressor()\n",
    "lgbm.fit(x_train, y_train)\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [lr, rf_reg, svm, mlp, cat_reg, lgbm,xgb]\n",
    "j=0\n",
    "for i in model_list:\n",
    "    print(model_name[j], ' training mean squared error is: ', mean_squared_error(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing mean squared error is: ', mean_squared_error(i.predict(x_test), y_test))\n",
    "    print(model_name[j], ' training mean absolute error is: ', mean_absolute_error(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing mean absolute error is: ', mean_absolute_error(i.predict(x_test), y_test))\n",
    "    print(model_name[j], ' training r-square is: ', r2_score(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing r-square is: ', r2_score(i.predict(x_test), y_test))\n",
    "    print()\n",
    "    j += 1\n",
    "\n",
    "print()\n",
    "j=0\n",
    "for i in model_list:\n",
    "    if i == rf_reg or i == svm:\n",
    "        results  = cross_validate(i, df_airbnb.drop('price',axis=1), df_airbnb['price'].ravel(), scoring='r2', cv=5, return_train_score = True)\n",
    "        print(model_name[j], ' cross val training mse:', sum(results['train_score'])/len(results['train_score']))\n",
    "        print(model_name[j], ' cross val testing mse:', sum(results['test_score'])/len(results['test_score']))\n",
    "    else:\n",
    "        results  = cross_validate(i, df_airbnb.drop('price',axis=1), df_airbnb['price'], scoring='r2', cv=5, return_train_score = True)\n",
    "        print(model_name[j], ' cross val training mse:', sum(results['train_score'])/len(results['train_score']))\n",
    "        print(model_name[j], ' cross val testing mse:', sum(results['test_score'])/len(results['test_score']))\n",
    "    print()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [40, 50, 60],\n",
    "    'max_depth': [10, 12],\n",
    "    'min_samples_split': [2, 5],\n",
    "    \"min_samples_leaf\" : [3, 5, 8]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf_reg, param_grid=param_grid, cv=5, scoring='r2')\n",
    "\n",
    "gs = gs.fit(df_airbnb.drop('price', axis= 1), df_airbnb['price'])\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(max_depth=12,min_samples_leaf=5,min_samples_split=2,n_estimators=60)\n",
    "rf_reg.fit(x_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Regressor Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reg = CatBoostRegressor(random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'depth': [3, 5, 7],\n",
    "    'l2_leaf_reg': [3, 5, 7],\n",
    "    'iterations': [300, 400],\n",
    "    'boosting_type': ['Ordered', 'Plain']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=cat_reg, param_grid=param_grid, cv=5, scoring='r2')\n",
    "\n",
    "gs = gs.fit(df_airbnb.drop('price', axis= 1), df_airbnb['price'])\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reg = CatBoostRegressor(boosting_type='Ordered',depth=7,iterations=400,l2_leaf_reg=5,learning_rate=0.1)\n",
    "cat_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Regressor Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='r2')\n",
    "\n",
    "gs = gs.fit(df_airbnb.drop('price', axis= 1), df_airbnb['price'])\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators= 100,max_depth= 7,learning_rate= 0.1, gamma= 0.2,min_child_weight=5 ,subsample=0.7 )\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [rf_reg, cat_reg, xgb]\n",
    "model_name = [\"Random Forest\", \"CatBoost\", \"XGBoost\"]\n",
    "j=0\n",
    "for i in model_list:\n",
    "    print(model_name[j], ' training mean squared error is: ', mean_squared_error(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing mean squared error is: ', mean_squared_error(i.predict(x_test), y_test))\n",
    "    print(model_name[j], ' training mean absolute error is: ', mean_absolute_error(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing mean absolute error is: ', mean_absolute_error(i.predict(x_test), y_test))\n",
    "    print(model_name[j], ' training r-square is: ', r2_score(i.predict(x_train), y_train))\n",
    "    print(model_name[j], ' testing r-square is: ', r2_score(i.predict(x_test), y_test))\n",
    "    print()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Airbnb Price Model - CatBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prices = pd.DataFrame({'Actual': np.expm1(y_test), 'Prediction': np.expm1(xgb.predict(x_test))})\n",
    "predicted_prices.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
